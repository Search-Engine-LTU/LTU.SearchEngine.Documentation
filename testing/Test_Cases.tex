\documentclass{article}

% --------------------
% Encoding & font
% --------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

% --------------------
% Page layout
% --------------------
\usepackage[a4paper, margin=2cm]{geometry}

% --------------------
% Title info
% --------------------
\title{Test Case Specification}
\author{Group 1}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
This document describes test cases for the LTU Search Engine project.
% ==========================================================
\section{Test Case: TC-FRQ-1003-A \newline Domain Rate and concurrency Limit Enforcement}

\subsection*{Related Requirements}
FRQ-1003

\subsection*{Description}
Verify that the crawler enforces per-domain rate limiting by ensuring that no more than 
\textit{maxConcurrencyPerDomain} concurrent fetch operations are executed for a single domain, 
and that consecutive fetch requests to the same domain are separated by at least 
\textit{minDelayMs}. The behavior is validated by observing the order, timing, and concurrency 
of fetch operations during a controlled crawl execution.


\subsection*{Preconditions}
\begin{itemize}
  \item The crawler is configured with specific values for \textit{maxConcurrencyPerDomain} and \textit{minDelayMs}.
  \item A test double for the page fetcher is available to record fetch start times and active fetch counts.
  \item A test-controlled time source is used to simulate the passage of time.
  \item A set of URLs belonging to the same domain is provided as crawl input.
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Initialize the crawler using the test-controlled time source and the instrumented page fetcher.
  \item Provide the crawler with multiple URLs belonging to the same domain.
  \item Start the crawling process.
  \item Record the start time and concurrency level for each fetch operation.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item The maximum number of simultaneous fetch operations for the domain does not exceed 
        \textit{maxConcurrencyPerDomain}.
  \item The time difference between consecutive fetch operations for the same domain is greater than or equal to 
        \textit{minDelayMs}.
  \item All fetch operations complete without errors.
\end{itemize}

% ==========================================================

% ==========================================================
\section{Test Case: TC-FRQ-1003-B \newline Whitelist Enforcement}

\subsection*{Related Requirements}
FRQ-1003

\subsection*{Description}
Verify that the crawler enforces domain whitelisting by only fetching pages whose domains 
are explicitly listed in the configured whitelist. URLs belonging to domains not present 
in the whitelist shall be ignored and no fetch attempts shall be performed for them.


\subsection*{Preconditions}
\begin{itemize}
  \item A domain whitelist containing one or more allowed domains is configured.
  \item A test double for the page fetcher is available to record attempted fetch operations.
  \item The crawler is provided with a set of URLs that includes:
    \begin{itemize}
      \item URLs belonging to whitelisted domains, and
      \item URLs belonging to non-whitelisted domains.
    \end{itemize}
\end{itemize}


\subsection*{Test Steps}
\begin{enumerate}
  \item Initialize the crawler with the configured domain whitelist and the instrumented page fetcher.
  \item Provide the crawler with the mixed set of whitelisted and non-whitelisted URLs.
  \item Start the crawling process.
  \item Record all fetch attempts performed by the crawler.
\end{enumerate}


\subsection*{Expected Results}
\begin{itemize}
  \item Fetch operations are performed only for URLs whose domains appear in the whitelist.
  \item No fetch attempts are made for URLs belonging to non-whitelisted domains.
  \item The crawl completes without errors.
\end{itemize}


% ==========================================================


% ==========================================================
\section{Test Case: TC-FRQ-1009}

\subsection*{Related Requirements}
FRQ-1009

\subsection*{Description}
Verify that the crawler detects hyperlinks that point to PDF files and
adds those PDF documents to the index so that they become searchable.

\subsection*{Preconditions}
\begin{itemize}
  \item Crawler component is operational
  \item Indexing component is operational
  \item Crawling of external resources (PDF files) is permitted
  \item At least one HTML page exists that contains a hyperlink to a PDF file
  \item The search index is initially empty.
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Configure the crawler with the HTML page as the seed URL.
  \item Start the crawling and indexing process.
  \item Allow the crawler to fetch the HTML page.
  \item Detect and follow the hyperlink to the linked PDF file.
  \item Allow the indexing component to process the discovered PDF file.
  \item Inspect the contents of the search index.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item The crawler detects the hyperlink pointing to the PDF document.
  \item The PDF document is scheduled for crawling or downloading.
  \item The PDF document is added to the index.
  \item Terms extracted from the PDF document become searchable.
  \item No errors occur as a result of handling the PDF file.
\end{itemize}


% ==========================================================
\section{Test Case: TC-FRQ-1010}

\subsection*{Related Requirements}
FRQ-1010

\subsection*{Description}
Verify that the crawler does not extract or follow hyperlinks contained
inside PDF documents. Hyperlinks embedded in PDF files must not be added
to the crawl frontier or visited, even if the crawler downloads the PDF file.

\subsection*{Preconditions}
\begin{itemize}
  \item Crawler component is operational
  \item Crawling of normal HTML hyperlinks is enabled
  \item A reachable PDF document exists that contains one or more hyperlinks
  \item Logging of visited and scheduled URLs is enabled
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Configure a seed URL that links to a PDF document.
  \item Start the crawler with default link-following enabled.
  \item Allow the crawler to access and download the PDF document.
  \item Inspect the crawl frontier (scheduled URL list).
  \item Inspect the list of visited URLs and crawler logs.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item The crawler may download or access the PDF file itself.
  \item No hyperlinks embedded inside the PDF are extracted.
  \item No hyperlinks embedded inside the PDF are scheduled for crawling.
  \item No hyperlinks embedded inside the PDF are visited.
  \item The crawler continues processing other allowed links normally.
  \item No errors or crashes occur as a result of encountering PDF links.
\end{itemize}

\subsection*{Expected Results}
\begin{itemize}
  \item Only visible textual content is extracted.
  \item Images, videos, scripts, and binary resources are ignored.
  \item No non-textual data is stored in the search index.
  \item Extracted text is suitable for term-based indexing.
\end{itemize}
% ==========================================================

% ==========================================================
\section{Test Case: TC-FRQ-2001}

\subsection*{Related Requirements}
FRQ-2001, FRQ-2004

\subsection*{Description}
Verify that the system extracts only textual content from HTML pages and ignores
all non-textual content during the indexing process.

\subsection*{Preconditions}
\begin{itemize}
  \item An HTML page containing visible text content
  \item The same page contains images, videos, scripts, and binary files
  \item Search index is empty
\end{itemize}
\subsection*{Test Steps}
\begin{enumerate}
  \item Submit the HTML page to the indexing component.
  \item Parse the HTML document.
  \item Extract all indexable content.
  \item Inspect extracted data before storage.
  \item Store extracted content in the search index.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item Only visible textual content is extracted.
  \item Images, videos, scripts, and binary resources are ignored.
  \item No non-textual data is stored in the search index.
  \item Extracted text is suitable for term-based indexing.
\end{itemize}
% ==========================================================
\section{Test Case: TC-FRQ-2002}

\subsection*{Related Requirements}
FRQ-2002

\subsection*{Description}
Verify that indexed terms are stored together with references to the pages
in which they appear using an inverted index structure.

\subsection*{Preconditions}
\begin{itemize}
  \item Two or more HTML pages containing overlapping keywords
  \item Search index is empty
\end{itemize}
\subsection*{Test Steps}
\begin{enumerate}
  \item Submit all pages to the indexing component.
  \item Extract and tokenize textual terms from each page.
  \item Normalize extracted terms.
  \item Store terms in the search index.
  \item Inspect the internal index structure.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item Each unique term is stored exactly once in the index.
  \item Each term maps to one or more page references (URLs or document IDs).
  \item Pages containing the same term are associated with that term.
  \item The index follows the inverted index model.
\end{itemize}
% ==========================================================
\section{Test Case: TC-FRQ-2003}

\subsection*{Related Requirements}
FRQ-2003

\subsection*{Description}
Verify that the system supports incremental updates of the search index
without rebuilding the entire index.

\subsection*{Preconditions}
\begin{itemize}
  \item Existing search index populated with indexed pages
  \item One existing page is modified or a new page is added
\end{itemize}
\subsection*{Test Steps}
\begin{enumerate}
  \item Run the indexing process on the initial dataset.
  \item Modify an existing page or add a new page.
  \item Run the indexing process again.
  \item Monitor which pages are re-indexed.
  \item Compare the index state before and after the update.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item Only new or modified pages are re-indexed.
  \item Unchanged indexed pages remain untouched.
  \item No full index rebuild occurs.
  \item The index remains consistent and searchable after the update.
\end{itemize}

% ==========================================================
\section{Test Case: TC-FRQ-3001}

\subsection*{Related Requirements}
TC-FRQ-3001

\subsection*{Description}
Verify that the search engine accepts and correctly processes search queries composed of simple terms and Boolean operators (e.g., AND).

\subsection*{Preconditions}
\begin{itemize}
  \item The search system is running and accessible.
  \item The search index is populated with test data containing known terms.
\begin{itemize}
        \item[] \textit{Example setup:}
        \begin{itemize}
            \item Document A contains only "cats".
            \item Document B contains only "dogs".
            \item Document C contains both "cats" and "dogs".
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Simple Term Search: Submit a search query with a single term: cats.
  \item Multiple Terms Search: Submit a search query with multiple terms: cats dogs.
  \item Operator Search: Submit a search query using the AND operator: cats AND dogs.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item Simple Term: The system returns Document A and Document C (all docs containing "cats").
  \item Multiple Terms: The system returns Document A, B, and C (depending on default ranking, but all relevant docs should be found).
  \item Operator Search: The system returns only Document C (the document containing both terms). This confirms the AND operator is working logically and not just finding any document with either word.
\end{itemize}
% ==========================================================

\end{document}
