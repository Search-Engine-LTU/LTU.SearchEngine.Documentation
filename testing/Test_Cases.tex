\documentclass{article}

% --------------------
% Encoding & font
% --------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

% --------------------
% Page layout
% --------------------
\usepackage[a4paper, margin=2cm]{geometry}

% --------------------
% Title info
% --------------------
\title{Test Case Specification}
\author{Group 1}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
This document describes test cases for the LTU Search Engine project.

% ==========================================================

\section{Test Case: TC-FRQ-1001}

\subsection*{Related Requirements}
FRQ-1001, FRQ-1002

\subsection*{Description}
Given a set of linked URLs, Verify that the crawler starts from a seed URL then finds and visits all reachable web pages recursively. 

\subsection*{Preconditions}
\begin{itemize}
  \item Crawler component is operational
  \item A search index surrogate is created and will store visited URL pages and the order of the visits.
  \item The search index surrogate is initially empty.
  \item A source seed URL, with accompanied HTML page, and at least two additional HTML pages outside of, but linked by the seed URL. And finally in one of the two last pages link to a final page. 
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Configure the crawler with the HTML page as the seed URL.
  \item Start the crawling process.
  \item Inspect the contents of the search index.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item No errors occur.
  \item The search index surrogate contains entries from the seed URL page and all other linked pages in recursively order.
\end{itemize}
% ==========================================================

\section{Test Case: TC-FRQ-1001-N1}

\subsection*{Related Requirements}
FRQ-1001

\subsection*{Description}
Verify that the crawler ignores a seed URL whose domain is not in the whitelist.

\subsection*{Preconditions}
\begin{itemize}
  \item The crawler system is operational.
  \item A domain whitelist configuration exists.
  \item The domain of the provided seed URL is not included in the whitelist.
  \item The current state of the search index can be observed.
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Record the current state of the search index.
  \item Issue a start crawl command using a seed URL whose domain is not part of the whitelist.
  \item Record the state of the search index after the command completes.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item The crawl request is terminated immediately.
  \item No crawling process or crawl workflow is initiated.
  \item No HTTP requests are sent.
  \item No URLs are enqueued for crawling.
  \item The state of the search index remains unchanged.
  \item The command completes without errors.
\end{itemize}

% ==========================================================

% ==========================================================
\section{Test Case: TC-FRQ-1003-A \newline Domain Rate and concurrency Limit Enforcement}

\subsection*{Related Requirements}
FRQ-1003

\subsection*{Description}
Verify that the crawler enforces per-domain rate limiting by ensuring that no more than 
\textit{maxConcurrencyPerDomain} concurrent fetch operations are executed for a single domain, 
and that consecutive fetch requests to the same domain are separated by at least 
\textit{minDelayMs}. The behavior is validated by observing the order, timing, and concurrency 
of fetch operations during a controlled crawl execution.


\subsection*{Preconditions}
\begin{itemize}
  \item The crawler is configured with specific values for \textit{maxConcurrencyPerDomain} and \textit{minDelayMs}.
  \item A test double for the page fetcher is available to record fetch start times and active fetch counts.
  \item A test-controlled time source is used to simulate the passage of time.
  \item A set of URLs belonging to the same domain is provided as crawl input.
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Initialize the crawler using the test-controlled time source and the instrumented page fetcher.
  \item Provide the crawler with multiple URLs belonging to the same domain.
  \item Start the crawling process.
  \item Record the start time and concurrency level for each fetch operation.
\end{enumerate}


\subsection*{Expected Results}
\begin{itemize}
  \item The maximum number of simultaneous fetch operations for the domain does not exceed 
        \textit{maxConcurrencyPerDomain}.
  \item The time difference between consecutive fetch operations for the same domain is greater than or equal to 
        \textit{minDelayMs}.
  \item All fetch operations complete without errors.
\end{itemize}
% ==========================================================

% ==========================================================
\section{Test Case: TC-FRQ-1003-B \newline Whitelist Enforcement}

\subsection*{Related Requirements}
FRQ-1003

\subsection*{Description}
Verify that the crawler enforces domain whitelisting by only fetching pages whose domains 
are explicitly listed in the configured whitelist. URLs belonging to domains not present 
in the whitelist shall be ignored and no fetch attempts shall be performed for them.


\subsection*{Preconditions}
\begin{itemize}
  \item A domain whitelist containing one or more allowed domains is configured.
  \item A test double for the page fetcher is available to record attempted fetch operations.
  \item The crawler is provided with a set of URLs that includes:
    \begin{itemize}
      \item URLs belonging to whitelisted domains, and
      \item URLs belonging to non-whitelisted domains.
    \end{itemize}
\end{itemize}


\subsection*{Test Steps}
\begin{enumerate}
  \item Initialize the crawler with the configured domain whitelist and the instrumented page fetcher.
  \item Provide the crawler with the mixed set of whitelisted and non-whitelisted URLs.
  \item Start the crawling process.
  \item Record all fetch attempts performed by the crawler.
\end{enumerate}


\subsection*{Expected Results}
\begin{itemize}
  \item Fetch operations are performed only for URLs whose domains appear in the whitelist.
  \item No fetch attempts are made for URLs belonging to non-whitelisted domains.
  \item The crawl completes without errors.
\end{itemize}

% ==========================================================

% ==========================================================
\section{Test Case: TC-FRQ-1004 \newline Logging maxConcurrencyPerDomain and minDelayMs at startup}

\subsection*{Related Requirements}
FRQ-1004

\subsection*{Description}
Verify that the crawler logs the active rate limiting configuration values 
\textit{maxConcurrencyPerDomain} and \textit{minDelayMs} during startup. 
The logged values shall reflect the effective configuration used by the crawler 
and be available for verification and debugging purposes.

\subsection*{Preconditions}
\begin{itemize}
  \item The crawler is configured with specific values for 
        \textit{maxConcurrencyPerDomain} and \textit{minDelayMs}.
  \item A test double or intercepting logger is available to capture log entries.
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Initialize the crawler with the specified rate limiting configuration.
  \item Start the crawler.
  \item Capture all log entries emitted during the startup phase.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item A log entry is produced during crawler startup.
  \item The log entry contains the configured value of \textit{maxConcurrencyPerDomain}.
  \item The log entry contains the configured value of \textit{minDelayMs}.
  \item The logged values match the effective configuration used by the crawler.
  \item No errors occur during startup.
\end{itemize}

% ==========================================================

% ==========================================================

\section{Test Case: TC-FRQ-1005 \newline Crawler Robots Compliance}

\subsection*{Related Requirements}
FRQ-1005

\subsection*{Description}
Verify that the crawler retrieves, parses, and enforces the rules defined in the \texttt{robots.txt} file
of a target domain. The crawler shall only request URLs that are explicitly allowed for its user-agent
and shall skip all URLs that are disallowed according to the robots policy.

\subsection*{Preconditions}
\begin{itemize}
  \item The crawler component is operational
  \item A robots.txt file is available at the root of the target domain
  \item The robots.txt file defines at least one \texttt{Disallow} rule for the crawler \texttt{user-agent}
  \item A seed URL belonging to the target domain is configured
  \item The seed page contains hyperlinks to both allowed and disallowed paths defined in \texttt{robots.txt}
  \item The crawler \texttt{user-agent} string is explicitly defined in the configuration
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Configure the crawler with a seed URL belonging to the target domain.
  \item Start the crawling process.
  \item Fetch and parse the \texttt{robots.txt} file for the domain.
  \item Attempt to crawl URLs that are both allowed and disallowed by the robots rules.
  \item Monitor crawler decisions for each attempted URL.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item The \texttt{robots.txt} file is fetched exactly once per domain.
  \item URLs disallowed by the robots rules are not requested.
  \item URLs allowed by the robots rules are requested and processed normally.
  \item All crawling decisions related to robots enforcement are logged.
\end{itemize}

% ==========================================================

% ==========================================================

% ==========================================================
\section{Test Case: TC-FRQ-1006}

\subsection*{Related Requirements}
FRQ-1006

\subsection*{Description}
Given a set of web pages containing duplicate links and multiple paths (both relative and absolute) leading to the same URL, verify that the crawler does not crawl (fetch/process) the same logical page more than once during a single execution.

\subsection*{Preconditions}
\begin{itemize}
  \item Crawler component is operational.
  \item A search index surrogate is created that records all fetch attempts (visited URLs) during execution.
  \item The search index surrogate is initially empty.
  \item A controlled test website (or local test server) exists with the following pages:
    \begin{itemize}
      \item Seed page \texttt{/index.html} linking to \texttt{/a.html} and \texttt{/b.html}.
      \item \texttt{/a.html} contains a relative link to \texttt{/c.html}.
      \item \texttt{/b.html} contains an absolute link to \texttt{http://test-domain/c.html} (duplicate target via different URL form).
      \item The seed page also contains two duplicate links to \texttt{/c.html}.
    \end{itemize}
  \item The crawler is configured to allow crawling of the test domain (e.g., domain whitelist includes the test host).
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Configure the crawler with \texttt{/index.html} as the seed URL.
  \item Start the crawling process and let it complete.
  \item Inspect the search index surrogate (or crawler log) for the list of visited URLs and/or fetch attempts.
  \item Count the number of times the logical page \texttt{/c.html} (after URL normalization) appears in the visited/fetched URL list.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item No errors occur.
  \item Each unique page is fetched/processed at most once during the execution, regardless of whether it is referenced via relative or absolute URLs.
  \item The page corresponding to \texttt{/c.html} appears exactly once in the visited/fetched URL list, even though it is linked multiple times and via multiple URL representations.
\end{itemize}

% ==========================================================

% ==========================================================
\section{Test Case: TC-FRQ-1007 \newline Crawler ignores non-relevant resources}

\subsection*{Related Requirements}
FRQ-1007

\subsection*{Description}
Verify that the crawler ignores non-relevant resources such as images, CSS files, and JavaScript files, 
and only processes HTML documents during crawling.

\subsection*{Preconditions}
\begin{itemize}
  \item The crawler component is operational.
  \item The crawler is configured with a seed URL pointing to an HTML page.
  \item The seed HTML page contains < a href="..." > links to:
    \begin{itemize}
      \item One or more HTML pages
      \item Image resources (e.g., \texttt{.png}, \texttt{.jpg})
      \item Stylesheet resources (e.g., \texttt{.css})
      \item Script resources (e.g., \texttt{.js})
    \end{itemize}
  \item A logging or indexing surrogate is available to record fetched URLs.
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Start the crawler with the configured seed URL.
  \item Allow the crawler to process all reachable links from the seed page.
  \item Inspect the recorded fetch attempts in the log or index surrogate.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item The crawler fetches and processes only HTML documents.
  \item No fetch attempts are made for image, CSS, or JavaScript resources.
  \item Non-relevant resource URLs do not appear in the crawl queue or index.
  \item The crawling process completes without errors.
\end{itemize}

% ==========================================================

% ==========================================================
\section{Test Case: TC-FRQ-1008 \newline Configurable whitelist update }

\subsection*{Related Requirements}
FRQ-1008

\subsection*{Description}
Verify that the system supports adding new domains to the whitelist via a configuration file 
and that the crawler allows crawling of newly added domains without requiring code changes or recompilation.

\subsection*{Preconditions}
\begin{itemize}
  \item The crawler component is operational.
  \item A domain whitelist configuration file exists.
  \item The whitelist file initially contains one allowed domain.
  \item A second domain is available but not present in the whitelist.
  \item Seed URLs are available for both domains.
  \item A logging or indexing surrogate is available to record visited domains.
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Start the crawler using a seed URL from the initially whitelisted domain.
  \item Verify that crawling proceeds for the initial domain.
  \item Stop the crawler.
  \item Add the second domain to the whitelist configuration file.
  \item Restart the crawler using a seed URL from the newly added domain.
  \item Observe the crawler behavior.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item The crawler allows crawling of the initially whitelisted domain.
  \item After updating the configuration file, the crawler allows crawling of the newly added domain.
  \item No code changes or recompilation are required to support the new domain.
  \item The crawler ignores domains not listed in the whitelist.
  \item No errors occur during startup or crawling.
\end{itemize}

% ==========================================================


% ==========================================================
\section{Test Case: TC-FRQ-1009-A (PDF Link Detection Only)}

\subsection*{Related Requirements}
FRQ-1009

\subsection*{Description}
Verify that the crawler detects hyperlinks in HTML pages that point to PDF files
and registers them as discovered PDF resources.

\subsection*{Preconditions}
\begin{itemize}
  \item Crawler component is operational.
  \item Crawling of external resources is permitted.
  \item A test HTML page exists containing at least one hyperlink to a PDF file.
  \item Robots.txt and domain allowlist permit access to both the HTML page and the PDF.
\end{itemize}

\subsection*{Test Data}
\begin{itemize}
  \item Seed URL: \texttt{https://example.test/page.html}
  \item HTML contains link: \texttt{<a href="https://example.test/docs/report.pdf">Report</a>}
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Configure the crawler with the HTML page as the seed URL.
  \item Start crawling.
  \item Allow the crawler to fetch and parse the HTML page.
  \item Observe the crawler's discovered links output.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item The crawler classifies the discovered URL as a PDF resource.
  \item The URL is tagged as PDF and enqueued for PDF processing.
  \item No indexing is performed in this test case.
\end{itemize}

% ==========================================================
\section{Test Case: TC-FRQ-1009-B (PDF Indexing)}

\subsection*{Related Requirements}
FRQ-1009

\subsection*{Description}
Verify that PDF documents discovered by the crawler are downloaded by the indexing
component, that their textual content is extracted, and that the documents become
searchable in the index.

\subsection*{Preconditions}
\begin{itemize}
  \item Crawler and indexing components are operational.
  \item PDF parsing and text extraction is enabled.
  \item The PDF is text-based (not scanned, no OCR required).
  \item Robots.txt and domain allowlist permit crawling of the HTML and the PDF.
  \item The search index is cleared before the test or a dedicated temporary test index is used.
\end{itemize}

\subsection*{Test Data}
\begin{itemize}
  \item Seed URL: HTML page containing a hyperlink to \texttt{sample.pdf}.
  \item PDF file contains unique term: \texttt{InformationRetrievalTestTerm}.
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Configure the crawler with the HTML page as the seed URL.
  \item Start the crawling process.
  \item Allow the crawler to discover the PDF link.
  \item Allow the indexing component to download and parse the PDF.
  \item Wait until crawl and indexing queues are empty.
  \item Perform a search for the unique term.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item The PDF is downloaded and parsed successfully.
  \item Extracted text is stored in the inverted index.
  \item Searching for \texttt{InformationRetrievalTestTerm} returns the PDF document exactly once.
  \item No crawl, parse, or index errors are logged.
\end{itemize}
% ==========================================================


% ==========================================================
\section{Test Case: TC-FRQ-1010}

\subsection*{Related Requirements}
FRQ-1010

\subsection*{Description}
Verify that the crawler does not extract or follow hyperlinks contained
inside PDF documents. Hyperlinks embedded in PDF files must not be added
to the crawl frontier or visited, even if the crawler downloads the PDF file.

\subsection*{Preconditions}
\begin{itemize}
  \item Crawler component is operational
  \item Crawling of normal HTML hyperlinks is enabled
  \item A reachable PDF document exists that contains one or more hyperlinks
  \item Logging of visited and scheduled URLs is enabled
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Configure a seed URL that links to a PDF document.
  \item Start the crawler with default link-following enabled.
  \item Allow the crawler to access and download the PDF document.
  \item Inspect the crawl frontier (scheduled URL list).
  \item Inspect the list of visited URLs and crawler logs.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item The crawler may download or access the PDF file itself.
  \item No hyperlinks embedded inside the PDF are extracted.
  \item No hyperlinks embedded inside the PDF are scheduled for crawling.
  \item No hyperlinks embedded inside the PDF are visited.
  \item The crawler continues processing other allowed links normally.
  \item No errors or crashes occur as a result of encountering PDF links.
\end{itemize}

% ==========================================================

% ==========================================================
\section{Test Case: TC-FRQ-2001}

\subsection*{Related Requirements}
FRQ-2001, FRQ-2004

\subsection*{Description}
Verify that the system extracts only textual content from HTML pages and ignores
all non-textual content during the indexing process.

\subsection*{Preconditions}
\begin{itemize}
  \item An HTML page containing visible text content
  \item The same page contains images, videos, scripts, and binary files
  \item Search index is empty
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Submit the HTML page to the indexing component.
  \item Parse the HTML document.
  \item Extract all indexable content.
  \item Inspect extracted data before storage.
  \item Store extracted content in the search index.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item Only visible textual content is extracted.
  \item Images, videos, scripts, and binary resources are ignored.
  \item No non-textual data is stored in the search index.
  \item Extracted text is suitable for term-based indexing.
\end{itemize}

% ==========================================================
\section{Test Case: TC-FRQ-2002}

\subsection*{Related Requirements}
FRQ-2002

\subsection*{Description}
Verify that indexed terms are stored together with references to the pages
in which they appear using an inverted index structure.

\subsection*{Preconditions}
\begin{itemize}
  \item Two or more HTML pages containing overlapping keywords
  \item Search index is empty
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Submit all pages to the indexing component.
  \item Extract and tokenize textual terms from each page.
  \item Normalize extracted terms.
  \item Store terms in the search index.
  \item Inspect the internal index structure.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item Each unique term is stored exactly once in the index.
  \item Each term maps to one or more page references (URLs or document IDs).
  \item Pages containing the same term are associated with that term.
  \item The index follows the inverted index model.
\end{itemize}

% ==========================================================
\section{Test Case: TC-FRQ-2003}

\subsection*{Related Requirements}
FRQ-2003

\subsection*{Description}
Verify that the system supports incremental updates of the search index
without rebuilding the entire index.

\subsection*{Preconditions}
\begin{itemize}
  \item Existing search index populated with indexed pages
  \item One existing page is modified or a new page is added
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Run the indexing process on the initial dataset.
  \item Modify an existing page or add a new page.
  \item Run the indexing process again.
  \item Monitor which pages are re-indexed.
  \item Compare the index state before and after the update.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item Only new or modified pages are re-indexed.
  \item Unchanged indexed pages remain untouched.
  \item No full index rebuild occurs.
  \item The index remains consistent and searchable after the update.
\end{itemize}

% ==========================================================

\section{Test Case: TC-FRQ-2005}

\subsection*{Related Requirements}
FRQ-2005

\subsection*{Description}
Verify that the system uses a clearly defined data structure (ER-diagram) and follows a specific operational order: 
a URL must be enqueued in the crawl queue before any fetch attempt, and a page record must be created only after the fetch attempt is completed.

\subsection*{Preconditions}
\begin{itemize}
  \item Crawler component is operational.
  \item A database or data structure following a defined ER-diagram is implemented.
  \item The crawl queue and page record table are initially empty.
  \item Monitoring of database transactions or system logs is enabled.
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Documentation Review: Inspect the system's technical 
  documentation to verify the existence of a clearly defined data structure/ER-diagram.
  \item Queue Inspection: Add a seed URL to the system and pause
   the process before the crawler performs the HTTP fetch.
  \item Verify Pre-fetch State: Check the crawl queue to confirm the URL is present.
  \item Verify Page Table: Confirm that no page record has been created for the URL yet.
  \item Complete Fetch: Allow the crawler to perform the fetch attempt (regardless of success or failure)
  \item Verify Post-fetch State: Inspect the database to confirm a page record is now created.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item A clearly defined data structure (e.g., ER-diagram) exists and matches the implementation.
  \item The URL is successfully enqueued before any fetch attempt is performed.
  \item A page record is created only after the fetch attempt is completed, even if the fetch was unsuccessful.
  \item The system maintains a consistent state between the crawl queue and the page records.
\end{itemize}


% ==========================================================

\section{Test Case: TC-FRQ-2006}

\subsection*{Related Requirements}
FRQ-2006

\subsection*{Description}
Verify, at a unit level, that the logic layer requests a page record creation strictly after the fetch attempt is finalized. This sequence must hold true even if the fetch component returns an error or throws an exception.

\subsection*{Preconditions}
\begin{itemize}
    \item The test isolates the \textbf{CrawlerService} (System Under Test).
    \item External dependencies (\textit{IFetcher}, \textit{IPageRepository}) are replaced with \textbf{Mock Objects}.
    \item The mock repository is configured to track calls to the \textit{CreateRecord} method.
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
    \item \textbf{Arrange (Success Scenario):} Configure the \textit{MockFetcher} to return a successful response (HTTP 200).
    \item \textbf{Act:} Call the crawl method on the \textit{CrawlerService} with a dummy URL.
    \item \textbf{Assert (Sequence):} Verify that \textit{MockRepository.CreateRecord()} was called once for this crawl invocation.
    \item \textbf{Assert (Order):} Verify that the fetch call occurred \textit{before} the create record call.
    \item \textbf{Arrange (Failure Scenario):} Reset mocks. Configure \textit{MockFetcher} to throw an exception (e.g., NetworkError).
    \item \textbf{Act:} Call the crawl method on the \textit{CrawlerService} with the same URL.
    \item \textbf{Assert (Resilience):} Verify that \textit{MockRepository.CreateRecord()} is still called once per invocation, ensuring the logic proceeds despite the fetch exception.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
    \item The service logic ensures the page record creation is attempted regardless of the fetch outcome (Success or Exception).
    \item No calls to the repository are made before the fetch attempt is initiated.
\end{itemize}

% ==========================================================
\section{Test Case: TC-FRQ-3001}

\subsection*{Related Requirements}
FRQ-3001

\subsection*{Description}
Verify that the search engine accepts and correctly processes search queries composed of simple terms and Boolean operators (e.g., AND).

\subsection*{Preconditions}
\begin{itemize}
  \item The search system is running and accessible.
  \item The search index is populated with test data containing known terms.
  \begin{itemize}
    \item[] \textit{Example setup:}
    \begin{itemize}
        \item Document A contains only "cats".
        \item Document B contains only "dogs".
        \item Document C contains both "cats" and "dogs".
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Simple Term Search: Submit a search query with a single term: cats.
  \item Multiple Terms Search: Submit a search query with multiple terms: cats dogs.
  \item Operator Search: Submit a search query using the AND operator: cats AND dogs.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item Simple Term: The system returns Document A and Document C (all docs containing "cats").
  \item Multiple Terms: The system returns Document A, B, and C (depending on default ranking, but all relevant docs should be found).
  \item Operator Search: The system returns only Document C (the document containing both terms). This confirms the AND operator is working logically and not just finding any document with either word.
\end{itemize}

% ==========================================================
\section{Test Case: TC-FRQ-3002}

\subsection*{Related Requirements}
FRQ-3002

\subsection*{Description}
Verify that the search engine correctly handles and retrieves results for a query consisting of a single, standalone word.

\subsection*{Preconditions}
\begin{itemize}
  \item The search system is running and the index is populated.
  \item[] \textit{Example setup:}
  \begin{itemize}
      \item Document A contains the word "test".
      \item Document B contains the word "hello".
      \item Document C contains "testing" (to verify exact match if applicable, or just distinct words).
  \end{itemize}
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
        \item Enter the single term \texttt{hello} into the search input.
        \item Execute the search.
        \item Clear results and enter the term \texttt{test}.
        \item Execute the search.
    \end{enumerate}


\subsection*{Expected Results}
\begin{itemize}
        \item For the query \texttt{hello}: Only Document B is returned.
        \item For the query \texttt{test}: Only Document A is returned.
       \item \textbf{Note:} Document C ("testing") should \textbf{not} be returned for the query "test", verifying that the search handles exact word boundaries correctly.
    \end{itemize}

% ==========================================================
\section{Test Case: TC-FRQ-3003}

\subsection*{Related Requirements}
FRQ-3003

\subsection*{Description}
Verify that the search engine correctly handles phrase queries consisting of multiple words enclosed in double quotation marks, and only returns documents where the exact phrase appears with the same word order and adjacency.

\subsection*{Preconditions}
\begin{itemize}
    \item The search system is running and the index is populated.
    \item[] \textit{Example setup:}
    \begin{itemize}
        \item Document A contains the exact phrase ``hello dolly''.
        \item Document B contains the words ``hello'' and ``dolly'' but not as a phrase (e.g., separated or in different order).
        \item Document C contains only one of the words (e.g., ``hello'').
    \end{itemize}
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
    \item Enter the query \texttt{"hello dolly"} (including quotation marks) into the search input.
    \item Execute the search.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
    \item Only Document A is returned, as it is the only one containing the exact phrase with correct word order and adjacency.
    \item Documents B and C are NOT returned[cite: 195, 196].
    \item The search handles exact word boundaries and sequence correctly[cite: 192].
\end{itemize}

% ==========================================================
\section{Test Case: TC-FRQ-3004}

\subsection*{Related Requirements}
FRQ-3004

\subsection*{Description}
Verify that Boolean operators are case-sensitive and are only recognized when specified in uppercase letters.

\subsection*{Preconditions}
\begin{itemize}
    \item The search system is running and the index is populated.
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
    \item Execute a query using uppercase Boolean operators (\texttt{AND}, \texttt{OR}, \texttt{NOT}).
    \item Execute equivalent queries using lowercase operators (\texttt{and}, \texttt{or}, \texttt{not}).
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
    \item Uppercase Boolean operators are interpreted according to Boolean logic.
    \item Lowercase operator tokens are not interpreted as Boolean operators.
\end{itemize}

% ==========================================================
\section{Test Case: TC-FRQ-3005}

\subsection*{Related Requirements}
FRQ-3005

\subsection*{Description}
Verify that the search engine applies Boolean OR semantics, matching documents containing at least one specified term, using all supported OR syntaxes.

\textit{Assumption:} Operator case sensitivity is enforced as defined in FRQ-3004.

\subsection*{Preconditions}
\begin{itemize}
    \item The search system is running and the index is populated.
    \item The index contains documents that include either, both, or none of the queried terms.
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
    \item Execute a query using the keyword operator \texttt{OR}.
    \item Execute an equivalent query using the symbol operator \texttt{||}.
    \item Execute an equivalent query using implicit OR via whitespace separation.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
    \item All query variants match documents containing at least one specified term.
    \item All OR syntaxes produce equivalent result sets.
\end{itemize}

% ==========================================================
\section{Test Case: TC-FRQ-3006}

\subsection*{Related Requirements}
FRQ-3006

\subsection*{Description}
Verify that the search engine applies Boolean AND semantics, matching only documents that contain all specified terms, using both supported AND syntaxes.

\subsection*{Preconditions}
\begin{itemize}
    \item The search system is running and the index is populated.
    \item The index contains documents with partial and complete term coverage.
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
    \item Execute a query using the keyword operator \texttt{AND}.
    \item Execute an equivalent query using the symbol operator \texttt{\&\&}.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
    \item Only documents containing all specified terms are matched.
    \item Both AND syntaxes produce equivalent result sets.
\end{itemize}


% ==========================================================
\section{Test Case: TC-FRQ-3007}

\subsection*{Related Requirements}
FRQ-3007

\subsection*{Description}
Verify that the search engine correctly supports the required (+) operator, ensuring that terms prefixed with the operator must exist in all matching documents.

\subsection*{Preconditions}
\begin{itemize}
  \item The search system is running and the index is populated.
  \item The search input is empty before the test begins.
  \item[] \textit{Test Data Setup:}
  \begin{itemize}
      \item Document A contains the exact phrase ``are cats'' and the word ``dog''.
      \item Document B contains the exact phrase ``are cats'' but does \textbf{not} contain the word ``dog''.
      \item Document C contains the word ``dog'' but does \textbf{not} contain the exact phrase ``are cats''.
      \item Document D contains neither the phrase ``are cats'' nor the word ``dog''.
  \end{itemize}
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
 \item Enter the query \texttt{+"are cats" dog} into the search input.
  \item Execute the search.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item Only documents containing the exact phrase ``are cats'' are returned.
  \item Documents A and B are returned, as both contain the required phrase.
  \item Document C is \textbf{not} returned, as it does not contain the required phrase.
  \item Document D is \textbf{not} returned, as it contains none of the query terms.
  \item The optional term ``dog'' may influence ranking (e.g., Document A may appear before Document B), but result ordering is \textbf{not} required to pass this test.
\end{itemize}

\section{Test Case: TC-FRQ-3009}

\subsection*{Related Requirements}
FRQ-3009

\subsection*{Description}
Verify that the search engine respects parentheses to control operator precedence, 
ensuring that grouped \newline expressions are evaluated as a single unit.

\subsection*{Preconditions}
\begin{itemize} 
  \item The search system is running and accessible. 
  \item The search index is populated with test data specifically designed to test logic overlap. 
  
  \begin{itemize} 
    \item[] \textit{Example setup:}  \begin{itemize} 
    \item Document A contains: "cat", "dog" (Matches the AND-group). 
    \item Document B contains: "fish" (Matches the OR-condition). 
    \item Document C contains: "cat", "bird" (Negative test: Has "cat" but misses "dog"). 
    \item Document D contains: "dog", "bird" (Negative test: Has "dog" but misses "cat"). 
  \end{itemize} Â  \end{itemize} 
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Run Grouped Query: Submit the search query: ("cat" AND "dog") OR "fish". 
  \item (Optional) Verify Precedence Change: Submit a contrasting query: "cat" AND ("dog" OR "fish").
 \end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
   \item Grouped Query Result: The system must return Document A (because it has cat+dog) and Document B (because it has fish). \item Exclusion Verification: The system must NOT return Document C or Document D. Even though they contain "cat" or "dog", they do not fulfill the specific (AND) condition required by the parentheses. \item (Optional) Precedence Result: If step 2 is run, the system should only return documents that have "cat" PLUS one of the other animals (drastically changing the result list).
 \end{itemize}
% ==========================================================

\section{Test Case: TC-FRQ-3010}

\subsection*{Related Requirements}
FRQ-3010, FRQ-3011

\subsection*{Description}
Verify that the query parser correctly supports escaping for \textbf{all} special characters defined in the requirements. 
The test utilizes a mock repository (or query builder inspection) 
to ensure that when these characters are preceded by a backslash, they are interpreted as literal characters rather than search operators.

\subsection*{Preconditions}
\begin{itemize}
    \item The search engine's query parser is isolated for unit testing.
    \item A mock object or spy is attached to the query builder to intercept the parsed query structure before execution.
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
    \item \textbf{Input:} Pass a single query string containing all supported escapable characters, each preceded by a backslash, to the query parser.
    \item[] \textit{Test String:} 
    \texttt{\textbackslash+ \textbackslash- \textbackslash\& \textbackslash| \textbackslash! \textbackslash( \textbackslash) \textbackslash\{ \textbackslash\} \textbackslash[ \textbackslash] \textbackslash\^{} \textbackslash" \textbackslash\textasciitilde \textbackslash* \textbackslash? \textbackslash: \textbackslash\textbackslash}
    \item \textbf{Inspect:} Examine the resulting query object or processed term string generated by the parser.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
    \item The parser processes the input without errors.
    \item The resulting query term consists of the characters stripped of their escape backslashes.
    \item \textbf{Expected Value:} \texttt{+ - \& | ! ( ) \{ \} [ ] \^{} " \textasciitilde{} * ? : \textbackslash}
    \item None of the characters are interpreted as boolean operators, grouping symbols, or field separators.
\end{itemize}

\end{document}
