\documentclass{article}

% --------------------
% Encoding & font
% --------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

% --------------------
% Page layout
% --------------------
\usepackage[a4paper, margin=2cm]{geometry}

% --------------------
% Title info
% --------------------
\title{Test Case Specification}
\author{Group 1}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
This document describes test cases for the LTU Search Engine project.

% ==========================================================
\section{Test Case: TC-FRQ-1001}

\subsection*{Related Requirements}
FRQ-1001

\subsection*{Description}
Given a set of linked URLs, Verify that the crawler starts from a seed URL then finds and visits all reachable web pages recursively. 

\subsection*{Preconditions}
\begin{itemize}
  \item Crawler component is operational
  \item A search index surrogate is created and will store visited URL pages and the order of the visits.
  \item The search index surrogate is initially empty.
  \item A source seed URL, with accompanied HTML page, and at least two additional HTML pages outside of, but linked by the seed URL. And finally in one of the two last pages link to a final page. 
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Configure the crawler with the HTML page as the seed URL.
  \item Start the crawling process.
  \item Inspect the contents of the search index.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item No errors occur.
  \item The search index surrogate contains entries from the seed URL page and all other linked pages in recursively order.
\end{itemize}

% ==========================================================


% ==========================================================
\section{Test Case: TC-FRQ-1003-A \newline Domain Rate and concurrency Limit Enforcement}

\subsection*{Related Requirements}
FRQ-1003

\subsection*{Description}
Verify that the crawler enforces per-domain rate limiting by ensuring that no more than 
\textit{maxConcurrencyPerDomain} concurrent fetch operations are executed for a single domain, 
and that consecutive fetch requests to the same domain are separated by at least 
\textit{minDelayMs}. The behavior is validated by observing the order, timing, and concurrency 
of fetch operations during a controlled crawl execution.


\subsection*{Preconditions}
\begin{itemize}
  \item The crawler is configured with specific values for \textit{maxConcurrencyPerDomain} and \textit{minDelayMs}.
  \item A test double for the page fetcher is available to record fetch start times and active fetch counts.
  \item A test-controlled time source is used to simulate the passage of time.
  \item A set of URLs belonging to the same domain is provided as crawl input.
\end{itemize}

\begin{enumerate}
  \item Initialize the crawler using the test-controlled time source and the instrumented page fetcher.
  \item Provide the crawler with multiple URLs belonging to the same domain.
  \item Start the crawling process.
  \item Record the start time and concurrency level for each fetch operation.
\end{enumerate}

\begin{itemize}
  \item The maximum number of simultaneous fetch operations for the domain does not exceed 
        \textit{maxConcurrencyPerDomain}.
  \item The time difference between consecutive fetch operations for the same domain is greater than or equal to 
        \textit{minDelayMs}.
  \item All fetch operations complete without errors.
\end{itemize}
% ==========================================================

% ==========================================================
\section{Test Case: TC-FRQ-1003-B \newline Whitelist Enforcement}

\subsection*{Related Requirements}
FRQ-1003

\subsection*{Description}
Verify that the crawler enforces domain whitelisting by only fetching pages whose domains 
are explicitly listed in the configured whitelist. URLs belonging to domains not present 
in the whitelist shall be ignored and no fetch attempts shall be performed for them.


\subsection*{Preconditions}
\begin{itemize}
  \item A domain whitelist containing one or more allowed domains is configured.
  \item A test double for the page fetcher is available to record attempted fetch operations.
  \item The crawler is provided with a set of URLs that includes:
    \begin{itemize}
      \item URLs belonging to whitelisted domains, and
      \item URLs belonging to non-whitelisted domains.
    \end{itemize}
\end{itemize}


\subsection*{Test Steps}
\begin{enumerate}
  \item Initialize the crawler with the configured domain whitelist and the instrumented page fetcher.
  \item Provide the crawler with the mixed set of whitelisted and non-whitelisted URLs.
  \item Start the crawling process.
  \item Record all fetch attempts performed by the crawler.
\end{enumerate}


\subsection*{Expected Results}
\begin{itemize}
  \item Fetch operations are performed only for URLs whose domains appear in the whitelist.
  \item No fetch attempts are made for URLs belonging to non-whitelisted domains.
  \item The crawl completes without errors.
\end{itemize}

% ==========================================================

% ==========================================================
\section{Test Case: TC-FRQ-1004 \newline Logging maxConcurrencyPerDomain and minDelayMs at startup}

\subsection*{Related Requirements}
FRQ-1004

\subsection*{Description}
Verify that the crawler logs the active rate limiting configuration values 
\textit{maxConcurrencyPerDomain} and \textit{minDelayMs} during startup. 
The logged values shall reflect the effective configuration used by the crawler 
and be available for verification and debugging purposes.

\subsection*{Preconditions}
\begin{itemize}
  \item The crawler is configured with specific values for 
        \textit{maxConcurrencyPerDomain} and \textit{minDelayMs}.
  \item A test double or intercepting logger is available to capture log entries.
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Initialize the crawler with the specified rate limiting configuration.
  \item Start the crawler.
  \item Capture all log entries emitted during the startup phase.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item A log entry is produced during crawler startup.
  \item The log entry contains the configured value of \textit{maxConcurrencyPerDomain}.
  \item The log entry contains the configured value of \textit{minDelayMs}.
  \item The logged values match the effective configuration used by the crawler.
  \item No errors occur during startup.
\end{itemize}

% ==========================================================

% ==========================================================

\section{Test Case: TC-FRQ-1005 \newline Crawler Robots Compliance}

\subsection*{Related Requirements}
FRQ-1005

\subsection*{Description}
Verify that the crawler retrieves, parses, and enforces the rules defined in the \texttt{robots.txt} file
of a target domain. The crawler shall only request URLs that are explicitly allowed for its user-agent
and shall skip all URLs that are disallowed according to the robots policy.

\subsection*{Preconditions}
\begin{itemize}
  \item The crawler component is operational
  \item A robots.txt file is available at the root of the target domain
  \item The robots.txt file defines at least one \texttt{Disallow} rule for the crawler \texttt{user-agent}
  \item A seed URL belonging to the target domain is configured
  \item The seed page contains hyperlinks to both allowed and disallowed paths defined in \texttt{robots.txt}
  \item The crawler \texttt{user-agent} string is explicitly defined in the configuration
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Configure the crawler with a seed URL belonging to the target domain.
  \item Start the crawling process.
  \item Fetch and parse the \texttt{robots.txt} file for the domain.
  \item Attempt to crawl URLs that are both allowed and disallowed by the robots rules.
  \item Monitor crawler decisions for each attempted URL.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item The \texttt{robots.txt} file is fetched exactly once per domain.
  \item URLs disallowed by the robots rules are not requested.
  \item URLs allowed by the robots rules are requested and processed normally.
  \item All crawling decisions related to robots enforcement are logged.
\end{itemize}

% ==========================================================

% ==========================================================
\section{Test Case: TC-FRQ-1009}

\subsection*{Related Requirements}
FRQ-1009

\subsection*{Description}
Verify that the crawler detects hyperlinks that point to PDF files and
adds those PDF documents to the index so that they become searchable.

\subsection*{Preconditions}
\begin{itemize}
  \item Crawler component is operational
  \item Indexing component is operational
  \item Crawling of external resources (PDF files) is permitted
  \item At least one HTML page exists that contains a hyperlink to a PDF file
  \item The search index is initially empty.
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Configure the crawler with the HTML page as the seed URL.
  \item Start the crawling and indexing process.
  \item Allow the crawler to fetch the HTML page.
  \item Detect and follow the hyperlink to the linked PDF file.
  \item Allow the indexing component to process the discovered PDF file.
  \item Inspect the contents of the search index.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item The crawler detects the hyperlink pointing to the PDF document.
  \item The PDF document is scheduled for crawling or downloading.
  \item The PDF document is added to the index.
  \item Terms extracted from the PDF document become searchable.
  \item No errors occur as a result of handling the PDF file.
\end{itemize}


% ==========================================================
\section{Test Case: TC-FRQ-1010}

\subsection*{Related Requirements}
FRQ-1010

\subsection*{Description}
Verify that the crawler does not extract or follow hyperlinks contained
inside PDF documents. Hyperlinks embedded in PDF files must not be added
to the crawl frontier or visited, even if the crawler downloads the PDF file.

\subsection*{Preconditions}
\begin{itemize}
  \item Crawler component is operational
  \item Crawling of normal HTML hyperlinks is enabled
  \item A reachable PDF document exists that contains one or more hyperlinks
  \item Logging of visited and scheduled URLs is enabled
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Configure a seed URL that links to a PDF document.
  \item Start the crawler with default link-following enabled.
  \item Allow the crawler to access and download the PDF document.
  \item Inspect the crawl frontier (scheduled URL list).
  \item Inspect the list of visited URLs and crawler logs.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item The crawler may download or access the PDF file itself.
  \item No hyperlinks embedded inside the PDF are extracted.
  \item No hyperlinks embedded inside the PDF are scheduled for crawling.
  \item No hyperlinks embedded inside the PDF are visited.
  \item The crawler continues processing other allowed links normally.
  \item No errors or crashes occur as a result of encountering PDF links.
\end{itemize}

\subsection*{Expected Results}
\begin{itemize}
  \item Only visible textual content is extracted.
  \item Images, videos, scripts, and binary resources are ignored.
  \item No non-textual data is stored in the search index.
  \item Extracted text is suitable for term-based indexing.
\end{itemize}
% ==========================================================

% ==========================================================
\section{Test Case: TC-FRQ-2001}

\subsection*{Related Requirements}
FRQ-2001, FRQ-2004

\subsection*{Description}
Verify that the system extracts only textual content from HTML pages and ignores
all non-textual content during the indexing process.

\subsection*{Preconditions}
\begin{itemize}
  \item An HTML page containing visible text content
  \item The same page contains images, videos, scripts, and binary files
  \item Search index is empty
\end{itemize}
\subsection*{Test Steps}
\begin{enumerate}
  \item Submit the HTML page to the indexing component.
  \item Parse the HTML document.
  \item Extract all indexable content.
  \item Inspect extracted data before storage.
  \item Store extracted content in the search index.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item Only visible textual content is extracted.
  \item Images, videos, scripts, and binary resources are ignored.
  \item No non-textual data is stored in the search index.
  \item Extracted text is suitable for term-based indexing.
\end{itemize}
% ==========================================================
\section{Test Case: TC-FRQ-2002}

\subsection*{Related Requirements}
FRQ-2002

\subsection*{Description}
Verify that indexed terms are stored together with references to the pages
in which they appear using an inverted index structure.

\subsection*{Preconditions}
\begin{itemize}
  \item Two or more HTML pages containing overlapping keywords
  \item Search index is empty
\end{itemize}
\subsection*{Test Steps}
\begin{enumerate}
  \item Submit all pages to the indexing component.
  \item Extract and tokenize textual terms from each page.
  \item Normalize extracted terms.
  \item Store terms in the search index.
  \item Inspect the internal index structure.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item Each unique term is stored exactly once in the index.
  \item Each term maps to one or more page references (URLs or document IDs).
  \item Pages containing the same term are associated with that term.
  \item The index follows the inverted index model.
\end{itemize}
% ==========================================================
\section{Test Case: TC-FRQ-2003}

\subsection*{Related Requirements}
FRQ-2003

\subsection*{Description}
Verify that the system supports incremental updates of the search index
without rebuilding the entire index.

\subsection*{Preconditions}
\begin{itemize}
  \item Existing search index populated with indexed pages
  \item One existing page is modified or a new page is added
\end{itemize}
\subsection*{Test Steps}
\begin{enumerate}
  \item Run the indexing process on the initial dataset.
  \item Modify an existing page or add a new page.
  \item Run the indexing process again.
  \item Monitor which pages are re-indexed.
  \item Compare the index state before and after the update.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item Only new or modified pages are re-indexed.
  \item Unchanged indexed pages remain untouched.
  \item No full index rebuild occurs.
  \item The index remains consistent and searchable after the update.
\end{itemize}

% ==========================================================
\section{Test Case: TC-FRQ-3001}

\subsection*{Related Requirements}
FRQ-3001

\subsection*{Description}
Verify that the search engine accepts and correctly processes search queries composed of simple terms and Boolean operators (e.g., AND).

\subsection*{Preconditions}
\begin{itemize}
  \item The search system is running and accessible.
  \item The search index is populated with test data containing known terms.
\begin{itemize}
        \item[] \textit{Example setup:}
        \begin{itemize}
            \item Document A contains only "cats".
            \item Document B contains only "dogs".
            \item Document C contains both "cats" and "dogs".
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
  \item Simple Term Search: Submit a search query with a single term: cats.
  \item Multiple Terms Search: Submit a search query with multiple terms: cats dogs.
  \item Operator Search: Submit a search query using the AND operator: cats AND dogs.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
  \item Simple Term: The system returns Document A and Document C (all docs containing "cats").
  \item Multiple Terms: The system returns Document A, B, and C (depending on default ranking, but all relevant docs should be found).
  \item Operator Search: The system returns only Document C (the document containing both terms). This confirms the AND operator is working logically and not just finding any document with either word.
\end{itemize}
% ==========================================================
\section{Test Case: TC-FRQ-3002}

\subsection*{Related Requirements}
FRQ-3002

\subsection*{Description}
Verify that the search engine correctly handles and retrieves results for a query consisting of a single, standalone word.

\subsection*{Preconditions}
\begin{itemize}
        \item The search system is running and the index is populated.
        \item[] \textit{Example setup:}
        \begin{itemize}
            \item Document A contains the word "test".
            \item Document B contains the word "hello".
            \item Document C contains "testing" (to verify exact match if applicable, or just distinct words).
        \end{itemize}
    \end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
        \item Enter the single term \texttt{hello} into the search input.
        \item Execute the search.
        \item Clear results and enter the term \texttt{test}.
        \item Execute the search.
    \end{enumerate}


\subsection*{Expected Results}
\begin{itemize}
        \item For the query \texttt{hello}: Only Document B is returned.
        \item For the query \texttt{test}: Only Document A is returned.
       \item \textbf{Note:} Document C ("testing") should \textbf{not} be returned for the query "test", verifying that the search handles exact word boundaries correctly.
    \end{itemize}
% ==========================================================
\section{Test Case: TC-FRQ-3006}

\subsection*{Related Requirements}
FRQ-3006

\subsection*{Description}
Verify that the search engine applies Boolean AND semantics, matching only documents that contain all specified terms, using both supported AND syntaxes.

\subsection*{Preconditions}
\begin{itemize}
    \item The search system is running and the index is populated.
    \item The index contains documents with partial and complete term coverage.
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
    \item Execute a query using the keyword operator \texttt{AND}.
    \item Execute an equivalent query using the symbol operator \texttt{\&\&}.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
    \item Only documents containing all specified terms are matched.
    \item Both AND syntaxes produce equivalent result sets.
\end{itemize}

    % ==========================================================
\section{Test Case: TC-FRQ-3010}

\subsection*{Related Requirements}
FRQ-3010, FRQ-3011

\subsection*{Description}
Verify that the query parser correctly supports escaping for \textbf{all} special characters defined in the requirements. 
The test utilizes a mock repository (or query builder inspection) 
to ensure that when these characters are preceded by a backslash, they are interpreted as literal characters rather than search operators.

\subsection*{Preconditions}
\begin{itemize}
    \item The search engine's query parser is isolated for unit testing.
    \item A mock object or spy is attached to the query builder to intercept the parsed query structure before execution.
\end{itemize}

\subsection*{Test Steps}
\begin{enumerate}
    \item \textbf{Input:} Pass a single query string containing all supported escapable characters, each preceded by a backslash, to the query parser.
    \item[] \textit{Test String:} 
    \texttt{\textbackslash+ \textbackslash- \textbackslash\& \textbackslash| \textbackslash! \textbackslash( \textbackslash) \textbackslash\{ \textbackslash\} \textbackslash[ \textbackslash] \textbackslash\^{} \textbackslash" \textbackslash\textasciitilde \textbackslash* \textbackslash? \textbackslash: \textbackslash\textbackslash}
    \item \textbf{Inspect:} Examine the resulting query object or processed term string generated by the parser.
\end{enumerate}

\subsection*{Expected Results}
\begin{itemize}
    \item The parser processes the input without errors.
    \item The resulting query term consists of the characters stripped of their escape backslashes.
    \item \textbf{Expected Value:} \texttt{+ - \& | ! ( ) \{ \} [ ] \^{} " \textasciitilde{} * ? : \textbackslash}
    \item None of the characters are interpreted as boolean operators, grouping symbols, or field separators.
\end{itemize}

\end{document}
